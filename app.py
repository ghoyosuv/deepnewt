# -*- coding: utf-8 -*-
"""AG-NE-AS-muestra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wSC66mcXgXwHYsaLy_7UijcvGlBHUsnj
"""

# import pandas as pd
import numpy as np
import random
import torch
from torch import nn
import copy
import pandas as pd
import sys

phiA = float(sys.argv[1])
phiB = float(sys.argv[2])
n_tournament = int(sys.argv[3])
ar_name = str(sys.argv[4])

"""# **AG-NE-AS settings**"""

SE = {"INDEX_IND": 0,
      "INPUT_LEN": [60, 60],
      "CNP_LAYERS_MIN_MAX": [1, 6],
      "FILTERS_LEN_MIN_MAX": [3, 6],
      "NUMBER_FEATURE_MAPS": [10, 50],
      "POOLING_LEN_MIN_MAX": [3, 6],
      "NUMBER_FILTERS_MIN_MAX": [1, 5],
      "OUTPUT_NEURONS": 3,
      "GENERATIONS": 100,
      "N_INDIVIDUALS": 20,
      "PHI_a": phiA,
      "PHI_b": phiB,
      "N_TOURNMENT": n_tournament,
      "N_SELECT": 2,
      "ADD_VALUE": [-2, 2],
      "G_PRIM": 3}

"""# **Algorithm Population**"""

def create_cnp(inpt_channels, filters_len, n_feature_maps, non_linear, pool_len, pool_op):
    # Convolution
    conv = nn.Conv2d(inpt_channels, n_feature_maps, filters_len, stride = 1)

    # Non-linear function
    nl = []
    if non_linear == "sigm":
        nl = nn.Sigmoid()
    elif non_linear == "tanh":
        nl = nn.Tanh()
    elif non_linear == "relu":
        nl = nn.ReLU()
    elif non_linear == "prelu":
        nl = nn.PReLU()
    else:
        print("ERROR: while creating non-linear in CNP (create_cnp/6 function)")
    
    # Pooling operation
    pool = []
    if pool_op == "avg":
        pool = nn.AvgPool2d(pool_len, stride = [1, 1])
    elif pool_op == "max":
        pool = nn.MaxPool2d(pool_len, stride = [1, 1])
    else:
        print("ERROR: while creating pooling operation in CNP (create/6 function)")

    return [conv, nl, pool]

def conv_out_dims(inpt, kernel_size):
    res1 = np.floor((inpt[0] + 0 - 1 * (kernel_size[0] - 1) - 1) + 1)
    res2 = np.floor((inpt[1] + 0 - 1 * (kernel_size[1] - 1) - 1) + 1)

    return [int(res1), int(res2)]

def pool_out_dims(inpt, kernel_size):
    res1 = np.floor((inpt[0] + 0 - 1 * (kernel_size[0] - 1) - 1) + 1)
    res2 = np.floor((inpt[1] + 0 - 1 * (kernel_size[1] - 1) - 1) + 1)

    return [int(res1), int(res2)]

def perm_values(non, pool):
    non = str(non)
    pool = str(pool)

    num = -1

    if non == "sigm" and pool == "max":
        num = 1
    elif non == "sigm" and pool == "avg":
        num = 2
    elif non == "tanh" and pool == "max":
        num = 3
    elif non == "tanh" and pool == "avg":
        num = 4
    elif non == "relu" and pool == "max":
        num = 5
    elif non == "relu" and pool == "avg":
        num = 6
    elif non == "prelu" and pool == "max":
        num = 7
    elif non == "prelu" and pool == "avg":
        num = 8
    else:
        print("ERROR: while creating a permutation number (perm_values/2 function)")
    
    return num

### This function creates de individuals
### The format is described in your notebook
def create_population(n, cnp_layers_min_max, filters_len_min_max, number_feature_maps_min_max, pooling_len_min_max, number_filters_min_max,
                      output_neurons, main_input):
    pop = {}

    # CNP common layers
    cnp_layers_min = cnp_layers_min_max[0]
    cnp_layers_max = cnp_layers_min_max[1]
    filter_len_min = filters_len_min_max[0]
    filter_len_max = filters_len_min_max[1]
    feature_maps_min = number_feature_maps_min_max[0]
    feature_maps_max = number_feature_maps_min_max[1]
    pooling_len_min = pooling_len_min_max[0]
    pooling_len_max = pooling_len_min_max[1]

    # Last-CNP layer
    number_filters_min = number_filters_min_max[0]
    number_filters_max = number_filters_min_max[1]

    for ind in range(n):
        ## Network type permutation
        perm = []

        #### ADD CNP LAYERS ####
        # Get number of CNP layers
        n_cl = np.random.randint(cnp_layers_min, cnp_layers_max + 1)
        cnp_layers = {}

        # Number of input channels
        inpt_channels = 1

        # Length of the input data
        input_len = main_input

        # If the number of CNP layers is not one, creates n_cnp - 1 layers
        if n_cl > 1:
            for layer in range(n_cl - 1):
                # Filters length
                x = np.random.randint(filter_len_min, filter_len_max + 1)
                y = np.random.randint(filter_len_min, filter_len_max + 1)
                filters_len = [x, y]

                # Get number of feature maps for filter
                n_feature_maps = np.random.randint(feature_maps_min, feature_maps_max + 1)

                # Get the non-linear function
                non_linear = np.random.choice(["tanh", "sigm", "relu", "prelu"])

                # Get the pooling operation
                pool_op = np.random.choice(["avg", "max"])

                # Permutation number
                idd = perm_values(non_linear, pool_op)
                perm.append(idd)

                # Pool length
                x = np.random.randint(pooling_len_min, pooling_len_max + 1)
                y = np.random.randint(pooling_len_min, pooling_len_max + 1)
                pool_len = [x, y]

                ### Creating the explicit CNP
                cnp = create_cnp(inpt_channels, filters_len, n_feature_maps, non_linear, pool_len, pool_op)
                
                ### Create tensor of inpt_channels, filters_len, n_feature_maps
                sum_mutation_w = np.zeros((n_feature_maps, inpt_channels, filters_len[0], filters_len[1]))
                sum_mutation_b = np.zeros(n_feature_maps)
                
                ### Calculating the size of convolution operation
                cos = conv_out_dims(input_len, filters_len)

                ### Calculating the size of pooling operation
                pos = pool_out_dims(cos, pool_len)

                cnp_layers[layer] = {"id": idd,
                                    "input_len": input_len,
                                    "filters_len": filters_len,
                                     "n_features_maps": n_feature_maps,
                                     "conv_out_size": cos,
                                     "non_linear": non_linear,
                                     "pool_op": pool_op,
                                     "pool_len": pool_len,
                                     "pool_out_size": pos,
                                     "cnp_exp": cnp,
                                     "sum_mutation_w": sum_mutation_w,
                                     "sum_mutation_b": sum_mutation_b}
                
                inpt_channels = n_feature_maps
                input_len = pos
        
        ### Create the last CNP layer  
        # Get number of filters
        n_filters = np.random.randint(number_filters_min, number_filters_max + 1)
        
        # Filters length
        filters_len = []
        
        for filter in range(n_filters):
            y = np.random.randint(filter_len_min, filter_len_max + 1)
            filters_len.append([y, input_len[1]])
        
        # Conv output size
        cos = [conv_out_dims(input_len, filter) for filter in filters_len]
        
        # Get number of feature maps for filter
        n_feature_maps = np.random.randint(feature_maps_min, feature_maps_max + 1)

        # Get the non-linear function
        non_linear = np.random.choice(["tanh", "sigm", "relu", "prelu"])

        # Get the pooling operation
        pool_op = np.random.choice(["avg", "max"])

        # Pool len
        pool_len = [[filter[0], 1] for filter in cos]

        ### Creating explicit convolutions
        convs_exp = [nn.Conv2d(inpt_channels, n_feature_maps, filter) for filter in filters_len]
        
        ### Creating sum mutations
        sum_mutations_w = [np.zeros((n_feature_maps, inpt_channels, filter[0], filter[1])) for filter in filters_len]
        sum_mutations_b = [np.zeros(n_feature_maps) for _ in range(n_filters)]
        
        ### Creating explicit non-linear
        nl = []
        if non_linear == "sigm":
            nl = nn.Sigmoid()
        elif non_linear == "tanh":
            nl = nn.Tanh()
        elif non_linear == "relu":
            nl = nn.ReLU()
        elif non_linear == "prelu":
            nl = nn.PReLU()
        else:
            print("ERROR: while creating non-linear in last-CNP (create_cnp/6 function)")

        ### Creating explicit poolings
        pools_exp = []
        if pool_op == "avg":
            pools_exp = [nn.AvgPool2d(pool, stride = [1, 1]) for pool in pool_len]
        elif pool_op == "max":
            pools_exp = [nn.MaxPool2d(pool, stride = [1, 1]) for pool in pool_len]
        else:
            print("ERROR: while creating pooling operation in last-CNP")

        # Id layer
        idd = perm_values(non_linear, pool_op)
        
        cnp_layers["last_cnp_layer"] = {"id": idd,
                                    "input_len": input_len,
                                    "n_filters": n_filters,
                                     "filters_len": filters_len,
                                     "convs_exp": convs_exp,
                                     "conv_out_size": cos,
                                     "n_features_maps": n_feature_maps,
                                     "non_linear": non_linear,
                                     "non_exp": nl,
                                     "pool_op": pool_op,
                                     "pool_len": pool_len,
                                     "pools_exp": pools_exp,
                                     "flatten_output_len": n_feature_maps * n_filters,
                                     "sum_mutations_w": sum_mutations_w,
                                     "sum_mutations_b": sum_mutations_b}

        #### ADD FC LAYER
        non_linear = np.random.choice(["tanh", "sigm", "softmax"])

        ### Creating explicit FC layer
        nlin = 0
        if non_linear == "tanh":
            nlin = nn.Tanh()
        elif non_linear == "sigm":
            nlin = nn.Sigmoid()
        elif non_linear == "softmax":
            nlin = nn.Softmax(dim = 1)
        else:
            print("ERROR: while creating non-linear in FC layer")

        fc_exp = [nn.Linear(n_feature_maps * n_filters, output_neurons), nlin]
        
        sum_mutation_w = np.zeros((output_neurons, n_feature_maps * n_filters))
        sum_mutation_b = np.zeros(3)

        pop[SE["INDEX_IND"]] = {"perm": perm,"n_cl": n_cl, "cnp_layers": cnp_layers, "fc_layer": {"fc_exp": fc_exp, "input": n_feature_maps * n_filters, "output": output_neurons, "non_linear": non_linear, "sum_mutation_w": sum_mutation_w, "sum_mutation_b": sum_mutation_b}}

        SE["INDEX_IND"] += 1
        
    return pop

"""# **Auxiliar functions**"""

def dbg(thing):
    print(thing)
    return thing

#### AUXILIARY FUNCTIONS ####
def intersection(lst1, lst2):
    lst3 = [value for value in lst1 if value in lst2]
    return list(set(lst3))

def find_similarity_numbers(parent1, parent2):
    layers1 = list(parent1["cnp_layers"].values())
    layers2 = list(parent2["cnp_layers"].values())

    # -1 to delete the "last_cnp"
    ids1 = [layer["id"] for layer in layers1[0 : len(layers1) - 1]]
    ids2 = [layer["id"] for layer in layers2[0 : len(layers2) - 1]]

    return intersection(ids1, ids2), ids1, ids2

def get_similarity_pooling(sim_num, ids1, ids2):
    if sim_num == []:
        return False
    else:
        indexes1, indexes2 = {}, {}

        for num in sim_num:
            indexes1[num] = list(np.where(np.asarray(ids1) == num)[0])
            indexes2[num] = list(np.where(np.asarray(ids2) == num)[0])

        selected1, selected2 = {}, {}

        for li, k in zip(list(indexes1.values()), list(indexes1.keys())):
            selc = -1

            if len(li) > 1:
                selc = random.choice(li)
            else:
                selc = li[0]
            
            selected1[k] = selc

        for li, k in zip(list(indexes2.values()), list(indexes2.keys())):
            selc = -1

            if len(li) > 1:
                selc = random.choice(li)
            else:
                selc = li[0]
            
            selected2[k] = selc
        
        # {similarity number: index in parent}
        return selected1, selected2

def change_parameters_cnp(individual):
    number_cnps = individual["n_cl"] - 1

    if number_cnps < 1:
        return

    for i in range(number_cnps):
        individual["cnp_layers"][i]["id"] = perm_values(individual["cnp_layers"][i]["non_linear"], individual["cnp_layers"][i]["pool_op"])
        individual["perm"][i] = individual["cnp_layers"][i]["id"]
        
        if i == 0:
            individual["cnp_layers"][i]["conv_out_size"] = conv_out_dims(individual["cnp_layers"][i]["input_len"], 
                                                                         individual["cnp_layers"][i]["filters_len"])
            individual["cnp_layers"][i]["pool_out_size"] = pool_out_dims(individual["cnp_layers"][i]["conv_out_size"],
                                                                         individual["cnp_layers"][i]["pool_len"])
            if i < number_cnps - 1:
                individual["cnp_layers"][i + 1]["input_len"] = individual["cnp_layers"][i]["pool_out_size"]
        else:
            individual["cnp_layers"][i]["conv_out_size"] = conv_out_dims(individual["cnp_layers"][i]["input_len"],
                                                                         individual["cnp_layers"][i]["filters_len"])
            individual["cnp_layers"][i]["pool_out_size"] = pool_out_dims(individual["cnp_layers"][i]["conv_out_size"],
                                                                         individual["cnp_layers"][i]["pool_len"])
            if i < number_cnps - 1:
                individual["cnp_layers"][i + 1]["input_len"] = individual["cnp_layers"][i]["pool_out_size"]

def change_parameters_deladd_last_cnp(ind, filters_len, pool_len, chans, max_cnp):
    # first poolings
    pool_op = ind["cnp_layers"]["last_cnp_layer"]["pool_op"]

    new_pooling_exp = []

    if pool_op == "max":
        new_pooling_exp = [nn.MaxPool2d(pool, stride = [1, 1]) for pool in pool_len]
    else:
        new_pooling_exp = [nn.AvgPool2d(pool, stride = [1, 1]) for pool in pool_len]

    ind["cnp_layers"]["last_cnp_layer"]["pools_exp"] = new_pooling_exp

    # channels for each conv filter
    last_channels_in = chans

    if max_cnp != []:
        last_channels_in = max_cnp["n_features_maps"]
    
    current_channels_in = ind["cnp_layers"]["last_cnp_layer"]["convs_exp"][0].in_channels
    dif_in_channels = current_channels_in - last_channels_in
    sum_mutations_w = copy.deepcopy(ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"])
    del ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"]
    
    for conv, x_i in zip(ind["cnp_layers"]["last_cnp_layer"]["convs_exp"], range(len(sum_mutations_w))):
        # Sum_mutations is a list, then it can be handled as a mutable variable
        x = sum_mutations_w[x_i]
        
        if dif_in_channels > 0:
            # del randomly channels
            for _ in range(0, dif_in_channels):
                selected_in_channel = np.random.randint(0, conv.in_channels)

                with torch.no_grad():
                    new_weight = np.delete(conv.weight, selected_in_channel, 1)
                    conv.weight = torch.nn.Parameter(new_weight)
                    
                conv.in_channels -= 1
                
                # Mutation sum
                x = np.delete(x, selected_in_channel, 1)
        elif dif_in_channels < 0:
            # add randomly channels
            for _ in range(0, dif_in_channels * (-1)):
                selected_in_channel = np.random.randint(0, conv.in_channels)

                first_part = conv.weight[:, 0:selected_in_channel, :, :]
                second_part = conv.weight[:, selected_in_channel:, :, :]
                insert_part = torch.rand(conv.out_channels, 1, conv.kernel_size[0], conv.kernel_size[1])

                with torch.no_grad():
                    new_weight = torch.cat((first_part, insert_part, second_part), 1)
                    conv.weight = torch.nn.Parameter(new_weight)
                        
                conv.in_channels += 1
                
                # Mutation sum
                fp = x[:, 0:selected_in_channel, :, :]
                sp = x[:, selected_in_channel:, :, :]
                ip = np.zeros((conv.out_channels, 1, conv.kernel_size[0], conv.kernel_size[1]))

                x = np.concatenate((fp, ip, sp), 1)
        
        sum_mutations_w[x_i] = copy.deepcopy(x)
        del x

    current_channels_in += dif_in_channels

    # dimensions of each conv filter
    for filt, filt_exp, x_i in zip(ind["cnp_layers"]["last_cnp_layer"]["filters_len"], ind["cnp_layers"]["last_cnp_layer"]["convs_exp"], range(len(sum_mutations_w))):
        dif_y = filt_exp.kernel_size[0] - filt[0]
        dif_x = filt_exp.kernel_size[1] - filt[1]
        
        x = sum_mutations_w[x_i]

        if dif_y > 0:
            # delete rows
            for _ in range(dif_y):
                selected_row = np.random.randint(0, filt_exp.kernel_size[0])

                with torch.no_grad():
                    new_conv = np.delete(filt_exp.weight, selected_row, 2)
                    filt_exp.weight = torch.nn.Parameter(new_conv)
                
                filt_exp.kernel_size = (filt_exp.kernel_size[0] - 1, filt_exp.kernel_size[1])
                
                # Mutation sum
                x = np.delete(x, selected_row, 2)
        elif dif_y < 0:
            # add rows
            for _ in range(dif_y * (-1)):
                selected_row = np.random.randint(0, filt_exp.kernel_size[0] + 1)

                first_half = filt_exp.weight[:, :, 0:selected_row, :]
                second_half = filt_exp.weight[:, :, selected_row:, :]

                insert_part = torch.rand(conv.out_channels, conv.in_channels, 1, filt_exp.kernel_size[1])

                with torch.no_grad():
                    new_conv = torch.cat((first_half, insert_part, second_half), 2)
                    filt_exp.weight = torch.nn.Parameter(new_conv)

                filt_exp.kernel_size = (filt_exp.kernel_size[0] + 1, filt_exp.kernel_size[1])
                
                # Mutation sum
                fp = x[:, :, 0:selected_row, :]
                sp = x[:, :, selected_row:, :]
                ip = np.zeros((conv.out_channels, conv.in_channels, 1, filt_exp.kernel_size[1]))

                x = np.concatenate((fp, ip, sp), 2)
            
        if dif_x > 0:
            # delete cols
            for _ in range(dif_x):
                selected_col = np.random.randint(0, filt_exp.kernel_size[1])

                with torch.no_grad():
                    new_conv = np.delete(filt_exp.weight, selected_col, 3)
                    filt_exp.weight = torch.nn.Parameter(new_conv)
                
                filt_exp.kernel_size = (filt_exp.kernel_size[0], filt_exp.kernel_size[1] - 1)
                
                # Mutation sum
                x = np.delete(x, selected_col, 3)
        elif dif_x < 0:
            # add cols
            for _ in range(dif_x * (-1)):
                selected_col = np.random.randint(0, filt_exp.kernel_size[1] + 1)

                first_half = filt_exp.weight[:, :, :, 0:selected_col]
                second_half = filt_exp.weight[:, :, :, selected_col:]

                insert_part = torch.rand(conv.out_channels, conv.in_channels, filt_exp.kernel_size[0], 1)

                with torch.no_grad():
                    new_conv = torch.cat((first_half, insert_part, second_half), 3)
                    filt_exp.weight = torch.nn.Parameter(new_conv)

                filt_exp.kernel_size = (filt_exp.kernel_size[0], filt_exp.kernel_size[1] + 1)
                
                # Mutation sum
                fp = x[:, :, :, 0:selected_col]
                sp = x[:, :, :, selected_col:]
                ip = np.zeros((conv.out_channels, conv.in_channels, filt_exp.kernel_size[0], 1))

                x = np.concatenate((fp, ip, sp), 3)
        
        sum_mutations_w[x_i] = copy.deepcopy(x)
        del x
        
                
    ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"] = copy.deepcopy(sum_mutations_w)
    del sum_mutations_w
    
def change_parameters_last_cnp(ind):
    cnp_layers_ids = list(ind["cnp_layers"].keys())

    del_ids = []

    for i in range(len(cnp_layers_ids)):
        if isinstance(cnp_layers_ids[i], str):
            del_ids.append(i)

    for ids in del_ids:
        del cnp_layers_ids[ids]

    if cnp_layers_ids != []:
        cnp_id_max = max(cnp_layers_ids)

        max_cnp = ind["cnp_layers"][cnp_id_max]

        last_output = max_cnp["pool_out_size"]
        last_channels = max_cnp["n_features_maps"]
        ind["cnp_layers"]["last_cnp_layer"]["input_len"] = last_output
        new_filters_len = [[filt[0], last_output[1]] for filt in ind["cnp_layers"]["last_cnp_layer"]["filters_len"]]
        new_conv_out_size = [conv_out_dims(last_output, filt) for filt in new_filters_len]

        ind["cnp_layers"]["last_cnp_layer"]["filters_len"] = new_filters_len
        ind["cnp_layers"]["last_cnp_layer"]["conv_out_size"] = new_conv_out_size
        new_pool_lens = ind["cnp_layers"]["last_cnp_layer"]["pool_len"] = copy.deepcopy(new_conv_out_size)

        ind["cnp_layers"]["last_cnp_layer"]["id"] = perm_values(ind["cnp_layers"]["last_cnp_layer"]["non_linear"], ind["cnp_layers"]["last_cnp_layer"]["pool_op"])

        change_parameters_deladd_last_cnp(ind, new_filters_len, new_pool_lens, last_channels, max_cnp)
    else:
        last_output = SE["INPUT_LEN"]
        last_channels = 1
        ind["cnp_layers"]["last_cnp_layer"]["input_len"] = last_output

        new_filters_len = [[filt[0], last_output[1]] for filt in ind["cnp_layers"]["last_cnp_layer"]["filters_len"]]
        new_conv_out_size = [conv_out_dims(last_output, filt) for filt in new_filters_len]

        ind["cnp_layers"]["last_cnp_layer"]["filters_len"] = new_filters_len
        ind["cnp_layers"]["last_cnp_layer"]["conv_out_size"] = new_conv_out_size
        new_pool_lens = ind["cnp_layers"]["last_cnp_layer"]["pool_len"] = copy.deepcopy(new_conv_out_size)

        ind["cnp_layers"]["last_cnp_layer"]["id"] = perm_values(ind["cnp_layers"]["last_cnp_layer"]["non_linear"], ind["cnp_layers"]["last_cnp_layer"]["pool_op"])

        change_parameters_deladd_last_cnp(ind, new_filters_len, new_pool_lens, last_channels, [])

def change_parameters_deladd_cnp(ind, last_input, last_ochan, ncnp, cnps):
    for cnp in range(ncnp, cnps):
        conv = ind["cnp_layers"][cnp]["cnp_exp"][0]
        conv_in_chan = conv.in_channels
        conv_out_chan = conv.out_channels
        dif_in_channels = conv_in_chan - last_ochan
        
        x = ind["cnp_layers"][cnp]["sum_mutation_w"]

        if dif_in_channels > 0:
            # del randomly channels
            for _ in range(0, dif_in_channels):
                selected_in_channel = np.random.randint(0, conv_in_chan)

                with torch.no_grad():
                    new_weight = np.delete(conv.weight, selected_in_channel, 1)
                    conv.weight = torch.nn.Parameter(new_weight)
                
                conv.in_channels -= 1
                conv_in_chan -= 1
                
                # Mutation sum
                x = np.delete(x, selected_in_channel, 1)
        elif dif_in_channels < 0:
            # add randomly channels
            for _ in range(0, dif_in_channels * (-1)):
                selected_in_channel = np.random.randint(0, conv_in_chan)

                first_part = conv.weight[:, 0:selected_in_channel, :, :]
                second_part = conv.weight[:, selected_in_channel:, :, :]
                insert_part = torch.rand(conv_out_chan, 1, conv.kernel_size[0], conv.kernel_size[1])

                with torch.no_grad():
                    new_weight = torch.cat((first_part, insert_part, second_part), 1)
                    conv.weight = torch.nn.Parameter(new_weight)
                    
                conv.in_channels += 1
                conv_in_chan += 1
                
                # Mutation sum
                fp = x[:, 0:selected_in_channel, :, :]
                sp = x[:, selected_in_channel:, :, :]
                ip = np.zeros((conv_out_chan, 1, conv.kernel_size[0], conv.kernel_size[1]))

                x = np.concatenate((fp, ip, sp), 1)

        last_ochan = conv_out_chan
        
        ind["cnp_layers"][cnp]["sum_mutation_w"] = copy.deepcopy(x)
        del x

    if ncnp != cnps:
        ind["cnp_layers"][ncnp]["input_len"] = last_input

def crossover1(sim_num, similarity_layers, parent1, parent2):
    # sl_from_parent is a dict:
    #   {similarity number: index in parent}
    sl_from_p1 = similarity_layers[0]
    sl_from_p2 = similarity_layers[1]

    cop_parent1 = copy.deepcopy(parent1)
    cop_parent2 = copy.deepcopy(parent2)

    to_cross1, to_cross2 = {}, {}
    for k, i in zip(sl_from_p1.keys(), sl_from_p1.values()):
        to_cross1[k] = {i: parent1["cnp_layers"][i]}

    for k, i in zip(sl_from_p2.keys(), sl_from_p2.values()):
        to_cross2[k] = {i: parent2["cnp_layers"][i]}

    index_k1 = []
    index_k2 = []

    for num in sim_num:
        layer1, layer2 = to_cross1[num], to_cross2[num]

        k1, k2 = list(layer1.keys())[0], list(layer2.keys())[0]

        pool1, pool2 = layer1[k1]["cnp_exp"][2], layer2[k2]["cnp_exp"][2]

        cop_parent1["cnp_layers"][k1]["cnp_exp"][2], cop_parent2["cnp_layers"][k2]["cnp_exp"][2] = pool2, pool1

        # Changing only dims of pooling op in current layer (k1 and k2) of both parents
        cop_parent1["cnp_layers"][k1]["pool_len"], cop_parent2["cnp_layers"][k2]["pool_len"] = pool2.kernel_size, pool1.kernel_size

        index_k1.append(k1)
        index_k2.append(k2)

    # Change parameters
    change_parameters_cnp(cop_parent1)
    change_parameters_cnp(cop_parent2)

    # Change parameters of last_cnp_layer
    change_parameters_last_cnp(cop_parent1)
    change_parameters_last_cnp(cop_parent2)

    return cop_parent1, cop_parent2

def change_parameters_fcl(ind):
    # print("entró")
    flatten_output_len = ind["cnp_layers"]["last_cnp_layer"]["flatten_output_len"]
    current_input_neurons = ind["fc_layer"]["input"]

    fcl = ind["fc_layer"]["fc_exp"][0]

    x = ind["fc_layer"]["sum_mutation_w"]

    dif_input = flatten_output_len - current_input_neurons

    if dif_input < 0:
        # delete weights
        for _ in range(-1 * dif_input):
            selected_weight = np.random.randint(0, current_input_neurons)

            with torch.no_grad():
                new_weight = np.delete(fcl.weight, selected_weight, 1)
                fcl.weight = torch.nn.Parameter(new_weight)
                
            # Mutation sum
            x = np.delete(x, selected_weight, 1)

            fcl.in_features -= 1
            current_input_neurons -= 1
            ind["fc_layer"]["input"] -= 1
    elif dif_input > 0:
        for _ in range(dif_input):
            selected_weight = np.random.randint(0, current_input_neurons + 1)

            first_half = fcl.weight[:, 0:selected_weight]
            second_half = fcl.weight[:, selected_weight:]

            insert_part = torch.rand(SE["OUTPUT_NEURONS"], 1)

            with torch.no_grad():
                new_weight = torch.cat((first_half, insert_part, second_half), 1)
                fcl.weight = torch.nn.Parameter(new_weight)
            
             # Mutation sum
            fp = x[:, 0:selected_weight]
            sp = x[:, selected_weight:]
            ip = np.zeros((SE["OUTPUT_NEURONS"], 1))

            x = np.concatenate((fp, ip, sp), 1)

            fcl.in_features += 1
            current_input_neurons += 1
            ind["fc_layer"]["input"] += 1
    
    del ind["fc_layer"]["sum_mutation_w"]
    ind["fc_layer"]["sum_mutation_w"] = copy.deepcopy(x)
    del x

"""# **Crossover operators**"""

#### CROSSOVER OPERATORS ####
# cnp_cx(population[id1], population[id2])
def cnp_cx(parent1, parent2):
    sim_num, ids1, ids2 = find_similarity_numbers(parent1, parent2)
    layers_to_cross = get_similarity_pooling(sim_num, ids1, ids2)
    
    offs1, offs2 = [], []

    if layers_to_cross:
        offs1, offs2 = crossover1(sim_num, layers_to_cross, parent1, parent2)
    else:
        offs1 = copy.deepcopy(parent2)
        offs2 = copy.deepcopy(parent1)
    
    return offs1, offs2

def last_cnp_cx(parent1, parent2):
    pool_op1 = parent1["cnp_layers"]["last_cnp_layer"]["pool_op"]
    pool_op2 = parent2["cnp_layers"]["last_cnp_layer"]["pool_op"]

    pool_lens1 = parent1["cnp_layers"]["last_cnp_layer"]["pool_len"]
    pool_lens2 = parent2["cnp_layers"]["last_cnp_layer"]["pool_len"]
    
    cop_parent1 = copy.deepcopy(parent1)
    cop_parent2 = copy.deepcopy(parent2)
    
    del parent1
    del parent2

    if pool_op2 != pool_op1:
        new_pools_exp1 = []
        if pool_op2 == "avg":
            new_pools_exp1 = [nn.AvgPool2d(pool, stride = [1, 1]) for pool in pool_lens1]
        elif pool_op2 == "max":
            new_pools_exp1 = [nn.MaxPool2d(pool, stride = [1, 1]) for pool in pool_lens1]
        else:
            print("ERROR: while creating pooling operation in last_cnp_cx parent1")
    
        new_pools_exp2 = []
        if pool_op1 == "avg":
            new_pools_exp2 = [nn.AvgPool2d(pool, stride = [1, 1]) for pool in pool_lens2]
        elif pool_op1 == "max":
            new_pools_exp2 = [nn.MaxPool2d(pool, stride = [1, 1]) for pool in pool_lens2]
        else:
            print("ERROR: while creating pooling operation in last_cnp_cx parent2")
        
        cop_parent1["cnp_layers"]["last_cnp_layer"]["pool_op"] = pool_op2
        cop_parent2["cnp_layers"]["last_cnp_layer"]["pool_op"] = pool_op1
        cop_parent1["cnp_layers"]["last_cnp_layer"]["pools_exp"] = new_pools_exp1
        cop_parent2["cnp_layers"]["last_cnp_layer"]["pools_exp"] = new_pools_exp2
    
    change_parameters_last_cnp(cop_parent1)
    change_parameters_last_cnp(cop_parent2)
    
    return cop_parent1, cop_parent2

def fc_cx(parent1, parent2):
    non1 = parent1["fc_layer"]["non_linear"]
    non2 = parent2["fc_layer"]["non_linear"]

    cop_parent1 = copy.deepcopy(parent1)
    cop_parent2 = copy.deepcopy(parent2)
    
    del parent1
    del parent2

    if non1 != non2:
        non_exp1 = cop_parent1["fc_layer"]["fc_exp"][1]
        non_exp2 = cop_parent2["fc_layer"]["fc_exp"][1]

        cop_parent1["fc_layer"]["non_linear"] = non2
        cop_parent2["fc_layer"]["non_linear"] = non1
        cop_parent1["fc_layer"]["fc_exp"][1] = non_exp2
        cop_parent2["fc_layer"]["fc_exp"][1] = non_exp1

    return cop_parent1, cop_parent2

"""# **Mutation operators**"""

def mutate_weights_conv_cnp(ind, current_generation):
    number_cnps = ind["n_cl"] - 1
    
    if number_cnps < 1:
        return
    
    selected_cnp = np.random.randint(0, number_cnps)
    conv = ind["cnp_layers"][selected_cnp]["cnp_exp"][0]
    x = ind["cnp_layers"][selected_cnp]["sum_mutation_w"]
    
    conv_sum = torch.FloatTensor(conv.weight.shape[0], conv.weight.shape[1], conv.weight.shape[2], conv.weight.shape[3]).uniform_(SE["ADD_VALUE"][0], SE["ADD_VALUE"][1])
    
    ones_rand = np.random.randint(2, size = (conv.weight.shape[0], conv.weight.shape[1], conv.weight.shape[2], conv.weight.shape[3]))

    """
    x_min = np.min(x)
    x_max = np.max(x)

    if current_generation != 0 and current_generation % SE.G_PRIM == 0 and x_min != x_max:
        res_x = []

        if np.random.randint(0, 2) != 1:
            # min
            res_x = x == x_min
        else:
            # max
            res_x = x < x_max

        ones_rand = np.logical_and(res_x, ones_rand).astype(int)
    """
    
    new_conv_weight = conv.weight + (conv_sum * torch.from_numpy(ones_rand))
    x += ones_rand

    with torch.no_grad():
        conv.weight = torch.nn.Parameter(new_conv_weight)
  
def mutate_bias_conv_cnp(ind, current_generation):
    number_cnps = ind["n_cl"] - 1
    
    if number_cnps < 1:
        return
    
    selected_cnp = np.random.randint(0, number_cnps)
    conv = ind["cnp_layers"][selected_cnp]["cnp_exp"][0]
    x = ind["cnp_layers"][selected_cnp]["sum_mutation_b"]
    
    conv_sum = torch.FloatTensor(conv.bias.shape[0]).uniform_(SE["ADD_VALUE"][0], SE["ADD_VALUE"][1])
    
    ones_rand = np.random.randint(2, size = (conv.bias.shape[0]))

    """
    x_min = np.min(x)
    x_max = np.max(x)

    if current_generation != 0 and current_generation % SE.G_PRIM == 0 and x_min != x_max:
        res_x = []

        if np.random.randint(0, 2) != 1:
            # min
            res_x = x == x_min
        else:
            # max
            res_x = x < x_max

        ones_rand = np.logical_and(res_x, ones_rand).astype(int)
    """

    new_conv_bias = conv.bias + (conv_sum * torch.from_numpy(ones_rand))
    x += ones_rand

    with torch.no_grad():
        conv.bias = torch.nn.Parameter(new_conv_bias)

def mutate_weights_conv_last_cnp(ind, current_generation):
    number_filters_last_cnp = ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
    
    selected_filter = 0
    
    if number_filters_last_cnp > 1:
        selected_filter = np.random.randint(0, number_filters_last_cnp)
    
    conv = ind["cnp_layers"]["last_cnp_layer"]["convs_exp"][selected_filter]
    x = ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"][selected_filter]
    
    conv_sum = torch.FloatTensor(conv.weight.shape[0], conv.weight.shape[1], conv.weight.shape[2], conv.weight.shape[3]).uniform_(SE["ADD_VALUE"][0], SE["ADD_VALUE"][1])
    
    ones_rand = np.random.randint(2, size = (conv.weight.shape[0], conv.weight.shape[1], conv.weight.shape[2], conv.weight.shape[3]))

    """
    x_min = np.min(x)
    x_max = np.max(x)

    if current_generation != 0 and current_generation % SE.G_PRIM == 0 and x_min != x_max:
        res_x = []

        if np.random.randint(0, 2) != 1:
            # min
            res_x = x == x_min
        else:
            # max
            res_x = x < x_max

        ones_rand = np.logical_and(res_x, ones_rand).astype(int)
    """

    new_conv_weight = conv.weight + (conv_sum * torch.from_numpy(ones_rand))
    x += ones_rand
    
    with torch.no_grad():
        conv.weight = torch.nn.Parameter(new_conv_weight)

def mutate_bias_conv_last_cnp(ind, current_generation):
    number_filters_last_cnp = ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
    
    selected_filter = 0
    
    if number_filters_last_cnp > 1:
        selected_filter = np.random.randint(0, number_filters_last_cnp)
    
    conv = ind["cnp_layers"]["last_cnp_layer"]["convs_exp"][selected_filter]
    x = ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_b"][selected_filter]
    
    conv_sum = torch.FloatTensor(conv.bias.shape[0]).uniform_(SE["ADD_VALUE"][0], SE["ADD_VALUE"][1])
    
    ones_rand = np.random.randint(2, size = (conv.bias.shape[0]))

    """
    x_min = np.min(x)
    x_max = np.max(x)

    if current_generation != 0 and current_generation % SE.G_PRIM == 0 and x_min != x_max:
        res_x = []

        if np.random.randint(0, 2) != 1:
            # min
            res_x = x == x_min
        else:
            # max
            res_x = x < x_max

        ones_rand = np.logical_and(res_x, ones_rand).astype(int)
    """

    new_conv_bias = conv.bias + (conv_sum * torch.from_numpy(ones_rand))
    x += ones_rand

    with torch.no_grad():
        conv.bias = torch.nn.Parameter(new_conv_bias)

def mutate_weights_fcl(ind, current_generation):
    fc_layer = ind["fc_layer"]["fc_exp"][0]
    x = ind["fc_layer"]["sum_mutation_w"]

    weight_sum = torch.FloatTensor(fc_layer.weight.shape[0], fc_layer.weight.shape[1]).uniform_(SE["ADD_VALUE"][0], SE["ADD_VALUE"][1])
    ones_rand = np.random.randint(2, size = (fc_layer.weight.shape[0], fc_layer.weight.shape[1]))

    x_min = np.min(x)
    x_max = np.max(x)

    """
    if current_generation != 0 and current_generation % SE.G_PRIM == 0 and x_min != x_max:
        res_x = []

        if np.random.randint(0, 2) != 1:
            # min
            res_x = x == x_min
        else:
            # max
            res_x = x < x_max

        ones_rand = np.logical_and(res_x, ones_rand).astype(int)
    """
    
    new_weight = fc_layer.weight + (weight_sum * torch.from_numpy(ones_rand))
    x += ones_rand
    
    with torch.no_grad():
        fc_layer.weight = torch.nn.Parameter(new_weight)

def mutate_bias_fcl(ind, current_generation):
    fc_layer = ind["fc_layer"]["fc_exp"][0]
    
    bias_sum = torch.FloatTensor(3).uniform_(SE["ADD_VALUE"][0], SE["ADD_VALUE"][1])
    
    x = ind["fc_layer"]["sum_mutation_b"]
    
    ones_rand = np.random.randint(2, size = (3))

    """
    x_min = np.min(x)
    x_max = np.max(x)

    if current_generation != 0 and current_generation % SE.G_PRIM == 0 and x_min != x_max:
        res_x = []

        if np.random.randint(0, 2) != 1:
            # min
            res_x = x == x_min
        else:
            # max
            res_x = x < x_max

        ones_rand = np.logical_and(res_x, ones_rand).astype(int)
    """
    
    new_bias = fc_layer.bias + (bias_sum * torch.from_numpy(ones_rand))
    x += ones_rand
    
    with torch.no_grad():
        fc_layer.bias = torch.nn.Parameter(new_bias)

# foo(element)
def mutate_xy_conv_cnp(ind):
    number_cnps = ind["n_cl"] - 1
    
    if number_cnps < 1:
        return
    
    selected_cnp = np.random.randint(0, number_cnps)
    conv = ind["cnp_layers"][selected_cnp]["cnp_exp"][0]

    conv_rows = conv.kernel_size[0]
    conv_cols = conv.kernel_size[1]
    number_filters = conv.weight.shape[1]
    number_chan = conv.weight.shape[0]
    
    x = ind["cnp_layers"][selected_cnp]["sum_mutation_w"]

    if np.random.randint(0, 2) != 1:
        # mutate rows
        if random.choice([-1, 1]) == 1 and conv_rows < SE["FILTERS_LEN_MIN_MAX"][0]:
            # randomly add a row to the filters
            selected_row = np.random.randint(0, conv_rows + 1)

            first_half = conv.weight[:, :, 0:selected_row, :]
            second_half = conv.weight[:, :, selected_row:, :]

            insert_part = torch.rand(number_chan, number_filters, 1, conv_cols)

            with torch.no_grad():
                new_conv = torch.cat((first_half, insert_part, second_half), 2)
                conv.weight = torch.nn.Parameter(new_conv)

            conv.kernel_size = (conv_rows + 1, conv_cols)
            
            # Mutation sums
            ip = np.zeros((number_chan, number_filters, 1, conv_cols))
            fh = x[:, :, 0:selected_row, :]
            sh = x[:, :, selected_row:, :]

            x = np.concatenate((fh, ip, sh), 2)
            
            ind["cnp_layers"][selected_cnp]["filters_len"] = [conv_rows + 1, conv_cols]
        elif conv_rows > SE["FILTERS_LEN_MIN_MAX"][0]:
            # randomly delete a row in the filters
            selected_row = np.random.randint(0, conv_rows)

            with torch.no_grad():
                new_conv = np.delete(conv.weight, selected_row, 2)
                conv.weight = torch.nn.Parameter(new_conv)
            
            conv.kernel_size = (conv_rows - 1, conv_cols)
            
            # Mutation sums
            x = np.delete(x, selected_row, 2)

            ind["cnp_layers"][selected_cnp]["filters_len"] = [conv_rows - 1, conv_cols]
    else:
        # mutate columns
        if random.choice([-1, 1]) == 1 and conv_cols < SE["FILTERS_LEN_MIN_MAX"][1]:
            # randomly add a column to the filters
            selected_col = np.random.randint(0, conv_cols + 1)

            first_half = conv.weight[:, :, :, 0:selected_col]
            second_half = conv.weight[:, :, :, selected_col:]

            insert_part = torch.rand(number_chan, number_filters, conv_rows, 1)

            with torch.no_grad():
                new_conv = torch.cat((first_half, insert_part, second_half), 3)
                conv.weight = torch.nn.Parameter(new_conv)

            conv.kernel_size = (conv_rows, conv_cols + 1)
            
            # Mutation sums
            ip = np.zeros((number_chan, number_filters, conv_rows, 1))
            fh = x[:, :, :, 0:selected_col]
            sh = x[:, :, :, selected_col:]

            x = np.concatenate((fh, ip, sh), 3)

            ind["cnp_layers"][selected_cnp]["filters_len"] = [conv_rows, conv_cols + 1]
        elif conv_cols > SE["FILTERS_LEN_MIN_MAX"][1]:
            # randomly delete a column in the filters
            selected_column = np.random.randint(0, conv_cols)
            
            with torch.no_grad():
                new_conv = np.delete(conv.weight, selected_column, 3)
                conv.weight = torch.nn.Parameter(new_conv)
            
            conv.kernel_size = (conv_rows, conv_cols - 1)
            
            # Mutation sums
            x = np.delete(x, selected_column, 3)

            ind["cnp_layers"][selected_cnp]["filters_len"] = [conv_rows, conv_cols - 1]
        
    del ind["cnp_layers"][selected_cnp]["sum_mutation_w"]
    ind["cnp_layers"][selected_cnp]["sum_mutation_w"] = copy.deepcopy(x)
    del x
        
    change_parameters_cnp(ind)
    change_parameters_last_cnp(ind)

def mutate_rows_conv_last_cnp(ind):
    number_filters_last_cnp = ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
    
    selected_filter = 0
    
    if number_filters_last_cnp > 1:
        selected_filter = np.random.randint(0, number_filters_last_cnp)
    
    conv = ind["cnp_layers"]["last_cnp_layer"]["convs_exp"][selected_filter]
    conv_rows = ind["cnp_layers"]["last_cnp_layer"]["filters_len"][selected_filter][0]
    conv_cols = conv.kernel_size[1]
    out_chan = conv.out_channels
    in_chan = conv.in_channels
    
    x = ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"][selected_filter]

    if random.choice([-1, 1]) == 1 and conv_rows > 1:
        # randomly delete a row in the filters
        selected_row = np.random.randint(0, conv_rows)

        with torch.no_grad():
            new_conv = np.delete(conv.weight, selected_row, 2)
            conv.weight = torch.nn.Parameter(new_conv)
            
        conv.kernel_size = (conv_rows - 1, conv_cols)
        
        # Mutation sums
        x = np.delete(x, selected_row, 2)
        
        ind["cnp_layers"]["last_cnp_layer"]["filters_len"][selected_filter][0] -= 1
    else:
        # randomly add a row to the filters
        selected_row = np.random.randint(0, conv_rows + 1)

        first_half = conv.weight[:, :, 0:selected_row, :]
        second_half = conv.weight[:, :, selected_row:, :]

        insert_part = torch.rand(out_chan, in_chan, 1, conv_cols)

        with torch.no_grad():
            new_conv = torch.cat((first_half, insert_part, second_half), 2)
            conv.weight = torch.nn.Parameter(new_conv)

        conv.kernel_size = (conv_rows + 1, conv_cols)
        
        # Mutation sums
        ip = np.zeros((out_chan, in_chan, 1, conv_cols))
        fh = x[:, :, 0:selected_row, :]
        sh = x[:, :, selected_row:, :]

        x = np.concatenate((fh, ip, sh), 2)
        
        ind["cnp_layers"]["last_cnp_layer"]["filters_len"][selected_filter][0] += 1
        
    ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"][selected_filter] = copy.deepcopy(x)
    del x
    
    change_parameters_last_cnp(ind)

def mutate_nonlinear(ind, type_mut):
    if type_mut == "cnp":
        number_cnps = ind["n_cl"] - 1
        
        if number_cnps < 1:
            return
    
        selected_cnp = np.random.randint(0, number_cnps)

        new_nonlinear = np.random.choice(["tanh", "sigm", "relu", "prelu"])

        ind["cnp_layers"][selected_cnp]["non_linear"] = new_nonlinear

        nl = []

        if new_nonlinear == "sigm":
            nl = nn.Sigmoid()
        elif new_nonlinear == "tanh":
            nl = nn.Tanh()
        elif new_nonlinear == "relu":
            nl = nn.ReLU()
        elif new_nonlinear == "prelu":
            nl = nn.PReLU()
        else:
            print("ERROR: while creating non-linear in CNP (mutate_nonlinear/2 function)")

        ind["cnp_layers"][selected_cnp]["cnp_exp"][1] = nl
        new_perm_value = perm_values(new_nonlinear, ind["cnp_layers"][selected_cnp]["pool_op"])
        ind["cnp_layers"][selected_cnp]["id"] = new_perm_value
        ind["perm"][selected_cnp] = new_perm_value
    else:
        non_linear = np.random.choice(["tanh", "sigm", "relu", "prelu"])

        ind["cnp_layers"]["last_cnp_layer"]["non_linear"] = non_linear

        nl = []
        
        if non_linear == "sigm":
            nl = nn.Sigmoid()
        elif non_linear == "tanh":
            nl = nn.Tanh()
        elif non_linear == "relu":
            nl = nn.ReLU()
        elif non_linear == "prelu":
            nl = nn.PReLU()
        else:
            print("ERROR: while creating non-linear in last-CNP (mutate_nonlinear/2 function)")

        ind["cnp_layers"]["last_cnp_layer"]["non_exp"] = nl
        ind["cnp_layers"]["last_cnp_layer"]["id"] = perm_values(non_linear, ind["cnp_layers"]["last_cnp_layer"]["pool_op"])

def mutate_xy_pool(ind):
    number_cnps = ind["n_cl"] - 1
    
    if number_cnps < 1:
        return
    
    selected_cnp = np.random.randint(0, number_cnps)

    pooling_len = ind["cnp_layers"][selected_cnp]["pool_len"]
    new_pool_len = copy.deepcopy(ind["cnp_layers"][selected_cnp]["pool_len"])

    if np.random.randint(0, 2) != 1:
        # mutate rows
        if random.choice([-1, 1]) == 1 and pooling_len[0] < SE["POOLING_LEN_MIN_MAX"][0]:
            # add a row
            new_pool_len[0] = pooling_len[0] + 1
        elif pooling_len[0] > SE["POOLING_LEN_MIN_MAX"][0]:
            # delete a row
            new_pool_len[0] = pooling_len[0] - 1
    else:
        if random.choice([-1, 1]) == 1 and pooling_len[1] < SE["POOLING_LEN_MIN_MAX"][1]:
            # add a column
            new_pool_len[1] = pooling_len[1] + 1
        elif pooling_len[1] > SE["POOLING_LEN_MIN_MAX"][1]:
            # delete a row
            new_pool_len[1] = pooling_len[1] - 1

    ind["cnp_layers"][selected_cnp]["pool_len"] = new_pool_len
    exp_pool = ind["cnp_layers"][selected_cnp]["pool_op"]

    pool = []
    
    if exp_pool == "avg":
        pool = nn.AvgPool2d(new_pool_len, stride = [1, 1])
    elif exp_pool == "max":
        pool = nn.MaxPool2d(new_pool_len, stride = [1, 1])
    else:
        print("ERROR: while creating pooling operation in CNP (mutate_xy_pool/1 function)")

    ind["cnp_layers"][selected_cnp]["cnp_exp"][2] = pool

    change_parameters_cnp(ind)
    change_parameters_last_cnp(ind)

def mutate_poolop(ind, type_mut):
    if type_mut == "cnp":
        number_cnps = ind["n_cl"] - 1
        
        if number_cnps < 1:
            return
    
        selected_cnp = np.random.randint(0, number_cnps)
        new_pool_op = []
        new_exp_pool = []
        pooling_len = ind["cnp_layers"][selected_cnp]["pool_len"]

        if ind["cnp_layers"][selected_cnp]["pool_op"] == "avg":
            new_pool_op = "max"
            new_exp_pool = nn.MaxPool2d(pooling_len, stride = [1, 1])
        else:
            new_pool_op = "avg"
            new_exp_pool = nn.AvgPool2d(pooling_len, stride = [1, 1])

        ind["cnp_layers"][selected_cnp]["pool_op"] = new_pool_op
        ind["cnp_layers"][selected_cnp]["cnp_exp"][2] = new_exp_pool
        new_perm_value = perm_values(ind["cnp_layers"][selected_cnp]["non_linear"], new_pool_op)
        ind["cnp_layers"][selected_cnp]["id"] = new_perm_value
        ind["perm"][selected_cnp] = new_perm_value
    else:
        pool_lens = ind["cnp_layers"]["last_cnp_layer"]["pool_len"]
        cur_pool_op = ind["cnp_layers"]["last_cnp_layer"]["pool_op"]
        new_pool_op = []
        new_exp_pool = []

        if cur_pool_op == "avg":
            new_pool_op = "max"
            for len_p in pool_lens:
                new_exp_pool.append(nn.MaxPool2d(len_p, stride = [1, 1]))
        else:
            new_pool_op = "avg"
            for len_p in pool_lens:
                new_exp_pool.append(nn.AvgPool2d(len_p, stride = [1, 1]))

        ind["cnp_layers"]["last_cnp_layer"]["pool_op"] = new_pool_op
        ind["cnp_layers"]["last_cnp_layer"]["pools_exp"] = new_exp_pool
        ind["cnp_layers"]["last_cnp_layer"]["id"] = perm_values(ind["cnp_layers"]["last_cnp_layer"]["non_linear"], new_pool_op)

def mutate_ncnp(ind):
    number_cnps = ind["n_cl"] - 1
    
    can_add_only = False

    selected_cnp = 0

    if number_cnps < 1:
        can_add_only = True
    else:
        selected_cnp = np.random.randint(0, number_cnps)

    if np.random.randint(0, 2) == 0 and number_cnps > 0:
        # delete
        input_len_del_cnp = ind["cnp_layers"][selected_cnp]["input_len"]
        del ind["cnp_layers"][selected_cnp]

        for cnp in range(selected_cnp + 1, number_cnps):
            ind["cnp_layers"][cnp - 1] = copy.deepcopy(ind["cnp_layers"][cnp])
            del ind["cnp_layers"][cnp]

        number_cnps -= 1
        ind["n_cl"] = number_cnps + 1
        
        last_out_channels = 1

        if number_cnps > 0 and selected_cnp != 0:
            last_out_channels = ind["cnp_layers"][selected_cnp - 1]["n_features_maps"]
        elif number_cnps == 0:
            last_out_channels = 1

        change_parameters_deladd_cnp(ind, input_len_del_cnp, last_out_channels, selected_cnp, number_cnps)
        
        # delete CNP ID from perm
        del ind["perm"][selected_cnp]

    elif number_cnps < SE["CNP_LAYERS_MIN_MAX"][1] - 1 and number_cnps != 0:
        # add
        previous_cnp_outsize = ind["cnp_layers"][selected_cnp]["pool_out_size"]
        inpt_channels = ind["cnp_layers"][selected_cnp]["n_features_maps"]

        filter_len_min = SE["FILTERS_LEN_MIN_MAX"][0]
        filter_len_max = SE["FILTERS_LEN_MIN_MAX"][1]
        feature_maps_min = SE["NUMBER_FEATURE_MAPS"][0]
        feature_maps_max = SE["NUMBER_FEATURE_MAPS"][1]
        pooling_len_min = SE["POOLING_LEN_MIN_MAX"][0]
        pooling_len_max = SE["POOLING_LEN_MIN_MAX"][1]

        x = np.random.randint(filter_len_min, filter_len_max + 1)
        y = np.random.randint(filter_len_min, filter_len_max + 1)
        filters_len = [x, y]

        # Get number of feature maps per filter
        n_feature_maps = np.random.randint(feature_maps_min, feature_maps_max + 1)

        # Get the non-linear function
        non_linear = np.random.choice(["tanh", "sigm", "relu", "prelu"])

        # Get the pooling operation
        pool_op = np.random.choice(["avg", "max"])

        # Permutation number
        idd = perm_values(non_linear, pool_op)

        # Pool length
        x = np.random.randint(pooling_len_min, pooling_len_max + 1)
        y = np.random.randint(pooling_len_min, pooling_len_max + 1)
        pool_len = [x, y]

        ### Creating the explicit CNP
        cnp = create_cnp(inpt_channels, filters_len, n_feature_maps, non_linear, pool_len, pool_op)
        
        ### Create tensor of inpt_channels, filters_len, n_feature_maps
        sum_mutation_w = np.zeros((n_feature_maps, inpt_channels, filters_len[0], filters_len[1]))
        sum_mutation_b = np.zeros(n_feature_maps)

        cos = conv_out_dims(previous_cnp_outsize, filters_len)

        ### Calculating the size of pooling operation
        pos = pool_out_dims(cos, pool_len)

        new_cnp_layer = {"id": idd,
                            "input_len": previous_cnp_outsize,
                            "filters_len": filters_len,
                            "n_features_maps": n_feature_maps,
                            "conv_out_size": cos,
                            "non_linear": non_linear,
                            "pool_op": pool_op,
                            "pool_len": pool_len,
                            "pool_out_size": pos,
                            "cnp_exp": cnp,
                            "sum_mutation_w": sum_mutation_w,
                            "sum_mutation_b": sum_mutation_b}

        for cnp in reversed(range(selected_cnp + 2, number_cnps + 1)):
            ind["cnp_layers"][cnp] = copy.deepcopy(ind["cnp_layers"][cnp - 1])
            del ind["cnp_layers"][cnp - 1]
            
        ind["cnp_layers"][selected_cnp + 1] = new_cnp_layer
        ind["n_cl"] += 1
        number_cnps += 1
        last_out_channels = ind["cnp_layers"][selected_cnp]["n_features_maps"]

        change_parameters_deladd_cnp(ind, previous_cnp_outsize, last_out_channels, selected_cnp + 1, number_cnps)
        
        # add CNP id to perm
        ind["perm"].insert(selected_cnp + 1, new_cnp_layer["id"])
    elif can_add_only:
        # add
        previous_cnp_outsize = SE["INPUT_LEN"]
        inpt_channels = 1

        filter_len_min = SE["FILTERS_LEN_MIN_MAX"][0]
        filter_len_max = SE["FILTERS_LEN_MIN_MAX"][1]
        feature_maps_min = SE["NUMBER_FEATURE_MAPS"][0]
        feature_maps_max = SE["NUMBER_FEATURE_MAPS"][1]
        pooling_len_min = SE["POOLING_LEN_MIN_MAX"][0]
        pooling_len_max = SE["POOLING_LEN_MIN_MAX"][1]

        x = np.random.randint(filter_len_min, filter_len_max + 1)
        y = np.random.randint(filter_len_min, filter_len_max + 1)
        filters_len = [x, y]

        # Get number of feature maps per filter
        n_feature_maps = np.random.randint(feature_maps_min, feature_maps_max + 1)

        # Get the non-linear function
        non_linear = np.random.choice(["tanh", "sigm", "relu", "prelu"])

        # Get the pooling operation
        pool_op = np.random.choice(["avg", "max"])

        # Permutation number
        idd = perm_values(non_linear, pool_op)

        # Pool length
        x = np.random.randint(pooling_len_min, pooling_len_max + 1)
        y = np.random.randint(pooling_len_min, pooling_len_max + 1)
        pool_len = [x, y]

        ### Creating the explicit CNP
        cnp = create_cnp(inpt_channels, filters_len, n_feature_maps, non_linear, pool_len, pool_op)
        
        ### Create tensor of inpt_channels, filters_len, n_feature_maps
        sum_mutation_w = np.zeros((n_feature_maps, inpt_channels, filters_len[0], filters_len[1]))
        sum_mutation_b = np.zeros(n_feature_maps)

        cos = conv_out_dims(previous_cnp_outsize, filters_len)

        ### Calculating the size of pooling operation
        pos = pool_out_dims(cos, pool_len)

        new_cnp_layer = {"id": idd,
                            "input_len": previous_cnp_outsize,
                            "filters_len": filters_len,
                            "n_features_maps": n_feature_maps,
                            "conv_out_size": cos,
                            "non_linear": non_linear,
                            "pool_op": pool_op,
                            "pool_len": pool_len,
                            "pool_out_size": pos,
                            "cnp_exp": cnp,
                            "sum_mutation_w": sum_mutation_w,
                            "sum_mutation_b": sum_mutation_b}

        ind["cnp_layers"][0] = new_cnp_layer
        ind["n_cl"] += 1
        number_cnps += 1

        # add CNP id to perm
        ind["perm"].append(new_cnp_layer["id"])
    
    change_parameters_cnp(ind)
    change_parameters_last_cnp(ind)

def mutate_out_channels_cnp(ind):
    number_cnps = ind["n_cl"] - 1
    
    if number_cnps < 1:
        return
    
    selected_cnp = np.random.randint(0, number_cnps)
    current_feature_maps = ind["cnp_layers"][selected_cnp]["n_features_maps"]
    dif_min = current_feature_maps - SE["NUMBER_FEATURE_MAPS"][0]
    dif_max = SE["NUMBER_FEATURE_MAPS"][1] - current_feature_maps
    
    features_add_del = 0
    
    if current_feature_maps == SE["NUMBER_FEATURE_MAPS"][0]:
        features_add_del = np.random.randint(0, SE["NUMBER_FEATURE_MAPS"][1] - SE["NUMBER_FEATURE_MAPS"][0])
    elif current_feature_maps == SE["NUMBER_FEATURE_MAPS"][1]:
        features_add_del = np.random.randint((SE["NUMBER_FEATURE_MAPS"][1] - SE["NUMBER_FEATURE_MAPS"][0]) * (- 1), 0)
    else:
        features_add_del = np.random.randint(-1 * dif_min, dif_max)
        
    new_features_maps = features_add_del + current_feature_maps
    ind["cnp_layers"][selected_cnp]["n_features_maps"] = new_features_maps
    conv = ind["cnp_layers"][selected_cnp]["cnp_exp"][0]
    out_size = ind["cnp_layers"][selected_cnp]["pool_out_size"]
    
    x_w = ind["cnp_layers"][selected_cnp]["sum_mutation_w"]
    x_b = ind["cnp_layers"][selected_cnp]["sum_mutation_b"]
    
    if features_add_del < 0:
        # delete output channels
        for _ in range(-1 * features_add_del):
            selected_out_chan = np.random.randint(0, current_feature_maps)
            
            with torch.no_grad():
                new_conv = np.delete(conv.weight, selected_out_chan, 0)
                conv.weight = torch.nn.Parameter(new_conv)
            
            with torch.no_grad():
                new_conv = np.delete(conv.bias, selected_out_chan, 0)
                conv.bias = torch.nn.Parameter(new_conv)
            
            conv.out_channels -= 1
            current_feature_maps -= 1
            
            # Mutation sum
            x_w = np.delete(x_w, selected_out_chan, 0)
            x_b = np.delete(x_b, selected_out_chan, 0)
        
        ind["cnp_layers"][selected_cnp]["sum_mutation_w"] = copy.deepcopy(x_w)
        ind["cnp_layers"][selected_cnp]["sum_mutation_b"] = copy.deepcopy(x_b)
        del x_w
        del x_b
        
        change_parameters_deladd_cnp(ind, out_size, current_feature_maps, selected_cnp + 1, number_cnps)
    elif features_add_del > 0:
        # add output channels
        for _ in range(features_add_del):
            selected_out_chan = np.random.randint(0, current_feature_maps + 1)
            
            # weight
            first_half = conv.weight[0:selected_out_chan, :, :, :]
            second_half = conv.weight[selected_out_chan:, :, :, :]
            
            insert_part = torch.rand(1, conv.in_channels, conv.kernel_size[0], conv.kernel_size[1])
            
            with torch.no_grad():
                new_conv = torch.cat((first_half, insert_part, second_half), 0)
                conv.weight = torch.nn.Parameter(new_conv)
            
            # bias
            first_half = conv.bias[0:selected_out_chan]
            second_half = conv.bias[selected_out_chan:]
            
            insert_part = torch.rand(1)
            
            with torch.no_grad():
                new_conv = torch.cat((first_half, insert_part, second_half), 0)
                conv.bias = torch.nn.Parameter(new_conv)
            
            conv.out_channels += 1
            current_feature_maps += 1
            
            # Mutation sum
            fh = x_w[0:selected_out_chan, :, :, :]
            sh = x_w[selected_out_chan:, :, :, :]
            ip = np.zeros((1, conv.in_channels, conv.kernel_size[0], conv.kernel_size[1]))

            x_w = np.concatenate((fh, ip, sh), 0)

            fh = x_b[0:selected_out_chan]
            sh = x_b[selected_out_chan:]
            ip = np.zeros(1)

            x_b = np.concatenate((fh, ip, sh), 0)
        
        ind["cnp_layers"][selected_cnp]["sum_mutation_w"] = copy.deepcopy(x_w)
        ind["cnp_layers"][selected_cnp]["sum_mutation_b"] = copy.deepcopy(x_b)
        del x_w
        del x_b
        
        change_parameters_deladd_cnp(ind, out_size, current_feature_maps, selected_cnp + 1, number_cnps)
        
    change_parameters_cnp(ind)
    change_parameters_last_cnp(ind)

def mutate_out_channels_last_cnp(ind):
    # print("aquí")
    current_feature_maps = ind["cnp_layers"]["last_cnp_layer"]["n_features_maps"]
    
    dif_min = current_feature_maps - SE["NUMBER_FEATURE_MAPS"][0]
    dif_max = SE["NUMBER_FEATURE_MAPS"][1] - current_feature_maps
    
    features_add_del = 0
    
    if current_feature_maps == SE["NUMBER_FEATURE_MAPS"][0]:
        features_add_del = np.random.randint(0, SE["NUMBER_FEATURE_MAPS"][1] - SE["NUMBER_FEATURE_MAPS"][0])
    elif current_feature_maps == SE["NUMBER_FEATURE_MAPS"][1]:
        features_add_del = np.random.randint((SE["NUMBER_FEATURE_MAPS"][1] - SE["NUMBER_FEATURE_MAPS"][0]) * (-1), 0)
    else:
        features_add_del = np.random.randint(-1 * dif_min, dif_max)
    
    new_features_maps = features_add_del + current_feature_maps
    
    ind["cnp_layers"]["last_cnp_layer"]["n_features_maps"] = new_features_maps
    convs = ind["cnp_layers"]["last_cnp_layer"]["convs_exp"]
    
    sum_mutations_w = ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"]
    sum_mutations_b = ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_b"]
    
    for conv, x_w, x_b in zip(convs, range(len(sum_mutations_w)), range(len(sum_mutations_b))):
        aux_current_feature_maps = current_feature_maps
        
        if features_add_del < 0:
            # delete output channels
            for _ in range(-1 * features_add_del):
                selected_out_chan = np.random.randint(0, aux_current_feature_maps)
                
                with torch.no_grad():
                    new_conv = np.delete(conv.weight, selected_out_chan, 0)
                    conv.weight = torch.nn.Parameter(new_conv)
                
                with torch.no_grad():
                    new_conv = np.delete(conv.bias, selected_out_chan, 0)
                    conv.bias = torch.nn.Parameter(new_conv)
                
                conv.out_channels -= 1
                aux_current_feature_maps -= 1
                
                # Mutation sums
                sum_mutations_w[x_w] = np.delete(sum_mutations_w[x_w], selected_out_chan, 0)
                sum_mutations_b[x_b] = np.delete(sum_mutations_b[x_b], selected_out_chan, 0)
            
        elif features_add_del > 0:
            # add output channels
            for _ in range(features_add_del):
                selected_out_chan = np.random.randint(0, aux_current_feature_maps + 1)
                
                first_half = conv.weight[0:selected_out_chan, :, :, :]
                second_half = conv.weight[selected_out_chan:, :, :, :]
                
                insert_part = torch.rand(1, conv.in_channels, conv.kernel_size[0], conv.kernel_size[1])
                
                with torch.no_grad():
                    new_conv = torch.cat((first_half, insert_part, second_half), 0)
                    conv.weight = torch.nn.Parameter(new_conv)
                
                first_half = conv.bias[0:selected_out_chan]
                second_half = conv.bias[selected_out_chan:]
                
                insert_part = torch.rand(1)
                
                with torch.no_grad():
                    new_conv = torch.cat((first_half, insert_part, second_half), 0)
                    conv.bias = torch.nn.Parameter(new_conv)
                
                conv.out_channels += 1
                aux_current_feature_maps += 1
                
                # Mutation sums
                fh = sum_mutations_w[x_w][0:selected_out_chan, :, :, :]
                sh = sum_mutations_w[x_w][selected_out_chan:, :, :, :]
                ip = np.zeros((1, conv.in_channels, conv.kernel_size[0], conv.kernel_size[1]))

                sum_mutations_w[x_w] = np.concatenate((fh, ip, sh), 0)

                fh = sum_mutations_b[x_b][0:selected_out_chan]
                sh = sum_mutations_b[x_b][selected_out_chan:]
                ip = np.zeros(1)

                sum_mutations_b[x_b] = np.concatenate((fh, ip, sh), 0)
    
    ind["cnp_layers"]["last_cnp_layer"]["flatten_output_len"] = new_features_maps * ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
    
    change_parameters_fcl(ind)

def mutate_nfilters_last_cnp(ind):
    # print("O aquí")
    features_maps = ind["cnp_layers"]["last_cnp_layer"]["n_features_maps"]
    number_filters_last_cnp = ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
    input_len = ind["cnp_layers"]["last_cnp_layer"]["input_len"]
    in_channels = ind["cnp_layers"]["last_cnp_layer"]["convs_exp"][0].in_channels
    pool_op = ind["cnp_layers"]["last_cnp_layer"]["pool_op"]

    can_delete = False
    if number_filters_last_cnp > 1:
        can_delete = True
    
    can_add = False
    if number_filters_last_cnp < SE["NUMBER_FILTERS_MIN_MAX"][1]:
        can_add = True
    
    if random.choice([-1, 1]) == 1 and can_delete:
        # delete a filter
        selected_filter = np.random.randint(0, number_filters_last_cnp)
        
        del ind["cnp_layers"]["last_cnp_layer"]["filters_len"][selected_filter]
        del ind["cnp_layers"]["last_cnp_layer"]["convs_exp"][selected_filter]
        del ind["cnp_layers"]["last_cnp_layer"]["conv_out_size"][selected_filter]
        del ind["cnp_layers"]["last_cnp_layer"]["pool_len"][selected_filter]
        del ind["cnp_layers"]["last_cnp_layer"]["pools_exp"][selected_filter]
        del ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"][selected_filter]
        del ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_b"][selected_filter]
        
        ind["cnp_layers"]["last_cnp_layer"]["n_filters"] -= 1
        ind["cnp_layers"]["last_cnp_layer"]["flatten_output_len"] = features_maps * ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
    elif can_add:
        # add a filter
        new_filter_len = [np.random.randint(SE["FILTERS_LEN_MIN_MAX"][0], SE["FILTERS_LEN_MIN_MAX"][1]),
                          input_len[1]]
        new_conv_exp = nn.Conv2d(in_channels, features_maps, new_filter_len)
        new_conv_out_size = conv_out_dims(input_len, new_filter_len)
        new_pool_len = copy.deepcopy(new_conv_out_size)
        
        new_pool_exp = []
        
        if pool_op == "max":
            new_pool_exp = nn.MaxPool2d(new_pool_len, stride = [1, 1])
        else:
            new_pool_exp = nn.AvgPool2d(new_pool_len, stride = [1, 1])
            
        # Mutation sums
        x_w = np.zeros((features_maps, in_channels, new_filter_len[0], new_filter_len[1]))
        x_b = np.zeros(features_maps)
        
        ind["cnp_layers"]["last_cnp_layer"]["filters_len"].append(new_filter_len)
        ind["cnp_layers"]["last_cnp_layer"]["convs_exp"].append(new_conv_exp)
        ind["cnp_layers"]["last_cnp_layer"]["conv_out_size"].append(new_conv_out_size)
        ind["cnp_layers"]["last_cnp_layer"]["pool_len"].append(new_pool_len)
        ind["cnp_layers"]["last_cnp_layer"]["pools_exp"].append(new_pool_exp)
        ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_w"].append(x_w)
        ind["cnp_layers"]["last_cnp_layer"]["sum_mutations_b"].append(x_b)
        
        ind["cnp_layers"]["last_cnp_layer"]["n_filters"] += 1
        ind["cnp_layers"]["last_cnp_layer"]["flatten_output_len"] = features_maps * ind["cnp_layers"]["last_cnp_layer"]["n_filters"]
        
    # modify number of input neurons from fc_l
    change_parameters_fcl(ind)

"""# **CNNs CPU execution**"""
def cnp_execution(inp, conv, non_linear, pooling):
    return pooling(non_linear(conv(inp)))

def last_cnp_execution(inp, convs, non_linear, pools):
    out_convs = [conv(inp) for conv in convs]

    out_nl = [non_linear(out) for out in out_convs]

    out_pools = [pool(out) for out, pool in zip(out_nl, pools)]

    return out_pools

def fc_layer_execution(inp, linear, non_linear):
    return non_linear(linear(inp))

def cnn_forward(inp, individual):
    cnp_layers = individual["cnp_layers"]
    last_layer = individual["fc_layer"]
    number_cnp_layers = individual["n_cl"] - 1
    
    x = inp
    
    for cnp in range(number_cnp_layers):
        x = cnp_execution(x, cnp_layers[cnp]["cnp_exp"][0], cnp_layers[cnp]["cnp_exp"][1], cnp_layers[cnp]["cnp_exp"][2])

    # This function, probably gets an error
    last_x = last_cnp_execution(x, cnp_layers["last_cnp_layer"]["convs_exp"], cnp_layers["last_cnp_layer"]["non_exp"], cnp_layers["last_cnp_layer"]["pools_exp"])
    
    first_cat = [ten for ten in last_x]

    x = torch.flatten(torch.cat(first_cat, 1), 1)

    x = fc_layer_execution(x, last_layer["fc_exp"][0], last_layer["fc_exp"][1])

    return x

def fitness_cpu(ind, data, target):
    count_corrects = 0

    for matrix, targ in zip(data, target):
        mat_torch = torch.from_numpy(matrix)
        mat_torch = torch.reshape(mat_torch, (1, 1, SE["INPUT_LEN"][0], SE["INPUT_LEN"][1])).to(dtype=torch.float)
        output_pred = cnn_forward(mat_torch, ind)

        if targ == torch.argmax(output_pred):
            count_corrects += 1

    ind["accuracy"] = count_corrects/len(target)

def fitness_pop_cpu(pop, data, target):
    for ind in pop:
        count_corrects = 0

        if ind == int(len(pop)/2):
            print("Fitness is in middle way...")

        for matrix, targ in zip(data, target):
            mat_torch = torch.from_numpy(matrix)
            mat_torch = torch.reshape(mat_torch, (1, 1, SE["INPUT_LEN"][0], SE["INPUT_LEN"][1])).to(dtype=torch.float)
            output_pred = cnn_forward(mat_torch, pop[ind])

            if targ == torch.argmax(output_pred):
                count_corrects += 1

        pop[ind]["accuracy"] = count_corrects/len(target)

    ord_pop = [{"id": p, "accuracy": pop[p]["accuracy"]} for p in pop]

    ord_pop = sorted(ord_pop, key = lambda i: i['accuracy'], reverse = True)

    return ord_pop

"""# **CNNs GPU execution**"""

def cnp_execution_gpu(inp, conv, non_linear, pooling):
    conv_clo = copy.deepcopy(conv).to("cuda")
    non_linear_clo = copy.deepcopy(non_linear).to("cuda")
    pooling_clo = copy.deepcopy(pooling).to("cuda")

    res = pooling_clo(non_linear_clo(conv_clo(inp)))

    del conv_clo
    del non_linear_clo
    del pooling_clo

    torch.cuda.empty_cache()

    return res

def last_cnp_execution_gpu(inp, convs, non_linear, pools):
    out_convs = []

    for conv in convs:
        conv_clo = copy.deepcopy(conv).to("cuda")
        out_convs.append(conv_clo(inp))

        del conv_clo
        torch.cuda.empty_cache()

    out_nl = []

    non_linear_clo = copy.deepcopy(non_linear).to("cuda")

    for out in out_convs:
        out_nl.append(non_linear_clo(out))
    
    del non_linear_clo
    torch.cuda.empty_cache()
    
    out_pools = []

    for out, pool in zip(out_nl, pools):
        pool_clo = copy.deepcopy(pool).to("cuda")
        out_pools.append(pool_clo(out))

        del pool_clo
        torch.cuda.empty_cache()

    return out_pools

def fc_layer_execution_gpu(inp, linear, non_linear):
    linear_clo = copy.deepcopy(linear).to("cuda")
    non_linear_clo = copy.deepcopy(non_linear).to("cuda")

    res = non_linear_clo(linear_clo(inp))

    del linear_clo
    del non_linear_clo
    
    torch.cuda.empty_cache()

    return res

def cnn_forward_gpu(inp, individual):
    cnp_layers = individual["cnp_layers"]
    last_layer = individual["fc_layer"]
    number_cnp_layers = individual["n_cl"] - 1

    x = inp.clone().to("cuda")

    for cnp in range(number_cnp_layers):
        x = cnp_execution_gpu(x, cnp_layers[cnp]["cnp_exp"][0], cnp_layers[cnp]["cnp_exp"][1], cnp_layers[cnp]["cnp_exp"][2])
    
    last_x = last_cnp_execution_gpu(x, cnp_layers["last_cnp_layer"]["convs_exp"], cnp_layers["last_cnp_layer"]["non_exp"], cnp_layers["last_cnp_layer"]["pools_exp"])
    
    first_cat = [ten for ten in last_x]

    x = torch.flatten(torch.cat(first_cat, 1), 1)
    for fi_ca in first_cat:
        del fi_ca
    
    torch.cuda.empty_cache()

    x = fc_layer_execution_gpu(x, last_layer["fc_exp"][0], last_layer["fc_exp"][1])

    x = x.to("cpu")

    return x

def fitness_gpu(ind, data, target):
    count_corrects = 0

    for matrix, targ in zip(data, target):
        mat_torch = torch.from_numpy(matrix)
        mat_torch = torch.reshape(mat_torch, (1, 1, SE["INPUT_LEN"][0], SE["INPUT_LEN"][1])).to(dtype=torch.float)
        output_pred = cnn_forward_gpu(mat_torch, ind)

        if targ == torch.argmax(output_pred):
            count_corrects += 1

    ind["accuracy"] = count_corrects/len(target)

def fitness_pop_gpu(pop, data, target):
    for ind in pop:
        count_corrects = 0

        if ind == int(len(pop)/2):
            print("Fitness is in middle way...")

        for matrix, targ in zip(data, target):
            mat_torch = torch.from_numpy(matrix)
            mat_torch = torch.reshape(mat_torch, (1, 1, SE["INPUT_LEN"][0], SE["INPUT_LEN"][1])).to(dtype=torch.float)
            output_pred = cnn_forward_gpu(mat_torch, pop[ind])

            if targ == torch.argmax(output_pred):
                count_corrects += 1

        pop[ind]["accuracy"] = count_corrects/len(target)

    ord_pop = [{"id": p, "accuracy": pop[p]["accuracy"]} for p in pop]

    ord_pop = sorted(ord_pop, key = lambda i: i['accuracy'], reverse = True)

    return ord_pop

"""# **Algorithm main**"""

def best_ind(pop):
    best_acc = -1
    best_id = -1

    for i in pop:
        if best_acc < pop[i]["accuracy"]:
            best_acc = pop[i]["accuracy"]
            best_id = i

    return {"best_id": best_id, "best_acc": best_acc}

def fitness_only(ind, data, target):
    count_corrects = 0

    for matrix, targ in zip(data, target):
        mat_torch = torch.from_numpy(matrix)
        mat_torch = torch.reshape(mat_torch, (1, 1, SE["INPUT_LEN"][0], SE["INPUT_LEN"][1])).to(dtype=torch.float)
        output_pred = cnn_forward_gpu(mat_torch, ind)

        if targ == torch.argmax(output_pred):
            count_corrects += 1

    return count_corrects/len(target)

def sort_by_accuracy(pop):
    ord_pop = [{"id": p, "accuracy": pop[p]["accuracy"]} for p in pop]

    ord_pop = sorted(ord_pop, key = lambda i: i['accuracy'], reverse = True)

    return ord_pop

def selection_main(pop_acc_ord):
    P_s = []

    while len(P_s) < SE["N_TOURNMENT"]:
        selected = np.random.choice(pop_acc_ord, SE["N_SELECT"], replace = False)

        max_acc = selected[0]["accuracy"]
        max_id = selected[0]["id"]

        for s in selected:
            if s["accuracy"] > max_acc:
                max_acc = s["accuracy"]
                max_id = s["id"]

        if not max_id in P_s:
            P_s.append(max_id)

    return P_s

def crossover_main(P_s, pop):
    i = 0

    P_c = []
    
    while i < len(P_s) - 1:
        off1_cnp, off2_cnp = cnp_cx(pop[P_s[i]], pop[P_s[i + 1]])
        off1_last_cnp, off2_last_cnp = last_cnp_cx(off1_cnp, off2_cnp)
        off1_fcl, off2_fcl = fc_cx(off1_last_cnp, off2_last_cnp)

        i += 2

        P_c.append(off1_fcl)
        P_c.append(off2_fcl)

    return P_c

def mutation_a_main(P_c, prob_a, c_gen):
    P_a = []
    
    for ind in P_c:
        ind_cop = copy.deepcopy(ind)

        mutated = False

        if np.random.uniform() < prob_a:
            mutate_nfilters_last_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_out_channels_last_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_out_channels_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_nonlinear(ind_cop, "cnp")
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_nonlinear(ind_cop, "last_cnp")
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_rows_conv_last_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_xy_conv_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_bias_fcl(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_weights_fcl(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_bias_conv_last_cnp(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_weights_conv_last_cnp(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_bias_conv_cnp(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_a:
            mutate_weights_conv_cnp(ind_cop, c_gen)
            mutated = True

        if mutated:
            P_a.append(ind_cop)
        else:
            del ind_cop

    return P_a

def mutation_b_main(P_a, prob_b, c_gen):
    P_b = []

    for ind in P_a:
        ind_cop = copy.deepcopy(ind)

        mutated = False

        if np.random.uniform() < prob_b:
            mutate_nfilters_last_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_out_channels_last_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_out_channels_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_ncnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_poolop(ind_cop, "cnp")
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_poolop(ind_cop, "last_cnp")
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_xy_pool(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_nonlinear(ind_cop, "cnp")
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_nonlinear(ind_cop, "last_cnp")
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_rows_conv_last_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_xy_conv_cnp(ind_cop)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_bias_fcl(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_weights_fcl(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_bias_conv_last_cnp(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_weights_conv_last_cnp(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_bias_conv_cnp(ind_cop, c_gen)
            mutated = True

        if np.random.uniform() < prob_b:
            mutate_weights_conv_cnp(ind_cop, c_gen)
            mutated = True

        if mutated:
            P_b.append(ind_cop)
        else:
            del ind_cop

    return P_b

def enumerate_new_inds(new_set):
    new_dic = {}

    for ind in new_set:
        new_dic[SE["INDEX_IND"]] = copy.deepcopy(ind)
        SE["INDEX_IND"] += 1

    del new_set

    return new_dic

def recombination(original_pop, P_c, P_a, P_b):
    P_c = enumerate_new_inds(P_c)
    P_a = enumerate_new_inds(P_a)
    P_b = enumerate_new_inds(P_b)

    original_pop.update(copy.deepcopy(P_c))
    original_pop.update(copy.deepcopy(P_a))
    original_pop.update(copy.deepcopy(P_b))

    list_sorted = sort_by_accuracy(original_pop)

    for delete_ind in list_sorted[SE["N_INDIVIDUALS"]:]:
        del original_pop[delete_ind["id"]]

    del P_c
    del P_a
    del P_b

"""# **Reading and preparing data**"""

sents = pd.read_csv("https://raw.githubusercontent.com/Cletsxd/tweets_exp_966/master/tweets/sents.csv")
tweets = pd.read_csv("https://raw.githubusercontent.com/Cletsxd/tweets_exp_966/master/tweets/tweets.csv")

sents_n = np.ndarray.flatten(sents["sent"].values)
tweets_n = np.ndarray.flatten(tweets["tweet"].values)

link = "https://raw.githubusercontent.com/Cletsxd/tweets_exp_966/master/tweets_or/"

matrix_word2vec = []
for sent, tweet in zip(sents_n, tweets_n):
    #print("Index:", tweet)
    matrix_word2vec.append(pd.read_csv(link + str(tweet) + "t_" + str(sent) + "s.csv").values)

tweets = np.asarray(matrix_word2vec)

sents = []
for sent in range(len(sents_n)):
    if sents_n[sent] == 3:
        sents.append(1)
    elif sents_n[sent] == 2:
        sents.append(0)
    elif sents_n[sent] == 1:
        sents.append(2)

"""# **MAIN**"""

random.seed(2)
np.random.seed(2)
torch.manual_seed(2)

population = []

population = create_population(SE["N_INDIVIDUALS"],
                                  SE["CNP_LAYERS_MIN_MAX"],
                                  SE["FILTERS_LEN_MIN_MAX"],
                                  SE["NUMBER_FEATURE_MAPS"],
                                  SE["POOLING_LEN_MIN_MAX"],
                                  SE["NUMBER_FILTERS_MIN_MAX"],
                                  SE["OUTPUT_NEURONS"],
                                  SE["INPUT_LEN"])

current_generation = 1

# If the 
fitness_pop = fitness_pop_cpu
fitness = fitness_cpu

if torch.cuda.is_available():
    if torch.cuda.device_count() > 0:
        fitness_pop = fitness_pop_gpu
        fitness = fitness_gpu

print("**********", ar_name, "**********")
#print("Running first fitness of individuals...")
ord_pop_ids_accs = fitness_pop(population, tweets, sents)

#print("Individuals are ready to evolve")

accuracies = []

while current_generation <= SE["GENERATIONS"]:
    #print("\nGeneration", current_generation)
    
    # best individual before operators
    #print(best_ind(population))
    accuracies.append(best_ind(population)["best_acc"])
    
    # P_s only ids from individuals
    P_s = selection_main(ord_pop_ids_accs)
    
    # P_c exp crossed individuals
    P_c = crossover_main(P_s, population)
    
    # P_a exp mutated via A movement individuals
    P_a = mutation_a_main(P_c, SE["PHI_a"], current_generation)
    
    # P_b exp mutated via B movement individuals
    P_b = mutation_b_main(P_a, SE["PHI_b"], current_generation)
    
    # Compute fitness of new individuals
    #print("Computing fitness P_c...")
    for ind in P_c:
        fitness(ind, tweets, sents)
    
    #print("Computing fitness P_a...")
    for ind in P_a:
        fitness(ind, tweets, sents)
    
    #print("Computing fitness P_b...")
    for ind in P_b:
        fitness(ind, tweets, sents)
    
    # recombination of better individuals    
    recombination(population, P_c, P_a, P_b)
    
    current_generation += 1
    
    ord_pop_ids_accs = sort_by_accuracy(population)
    
    #print("Individuals are ready to next generation...")

print("***************************** NEW instance", ar_name, "********************************************************")
print("Evolving has finished")
print("Best individual:\n", best_ind(population))

best_idd = best_ind(population)["best_id"]
print("Accuracy:", population[best_idd]["accuracy"])
print("cnp_layers:", population[best_idd]["cnp_layers"])
print("fc_layer:", population[best_idd]["fc_layer"])
print("n_cl:", population[best_idd]["n_cl"])
print("perm:",population[best_idd]["perm"])

print("\n Accuracies:\n", accuracies)
#print(best_ind(population))

#font_title = {'family' : 'serif', 
#        'color' : 'darkred', 
#        'weight' : 'normal',
#        'size' : 16}

#font_equations = {'family' : 'serif', 
#                  'color' : 'black',
#                  'weight' : 'normal',
#                  'size' : 12}

#font_labels = {'family' : 'serif', 
#                  'color' : 'black',
#                  'weight' : 'bold',
#                  'size' : 12}

#import matplotlib.pyplot as plt

#plt.plot(np.arange(100), accuracies, color = "b")
#plt.legend(["DeepNEWT"])
#plt.title("Accuracy convergence with DeepNEWT", fontdict = font_title)
#plt.ylabel("Accuracy (%)", fontdict = font_labels)
#plt.xlabel("Generations", fontdict = font_labels)
#plt.show()
